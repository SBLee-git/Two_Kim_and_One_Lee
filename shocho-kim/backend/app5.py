# 1. ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
from fastapi import APIRouter
from pydantic import BaseModel
from transformers import BertTokenizer, BertForSequenceClassification, GPT2LMHeadModel, AutoTokenizer
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
from evaluate import load
import numpy as np
import pandas as pd

# 2. ëª¨ë¸ ì„¤ì •í•˜ê¸°
model_name = "skt/kogpt2-base-v2"

# 3. í† í¬ë‚˜ì´ì € ì„¤ì •í•˜ê¸°
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token # íŒ¨ë”© í† í° ì„¤ì •

# 4. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class LyricsDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=128):
        self.texts = texts.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        inputs = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt",
        )
        inputs["labels"] = inputs["input_ids"].clone()
        return {key: val.squeeze(0).to("cuda") for key, val in inputs.items()}
    
# 5. ê¸°ì¡´ ê°€ì‚¬ ë°ì´í„° ë¡œë“œ
df = pd.read_csv("../../1.ë°ì´í„°ëª¨ìŒ/'music_data(Merge)'.csv").dropna()
df['sentences'] = df['lyrics'].apply(lambda x: [s.strip() for s in x.split("\n") if s.strip()])
lyrics_sentences = [sent for sublist in df['sentences'] for sent in sublist]

lyrics_texts = df["lyrics"]

# 6. í•™ìŠµ, ê²€ì¦ ë°ì´í„° ë¶„ë¦¬
train_texts, val_texts = train_test_split(lyrics_texts, test_size=0.2, random_state=42)
train_dataset = LyricsDataset(train_texts, tokenizer)
val_dataset = LyricsDataset(val_texts, tokenizer)

train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)

# 7. ëª¨ë¸ ë¡œë“œ ë° GPUë¡œ ì´ë™
model = GPT2LMHeadModel.from_pretrained(model_name)
model.resize_token_embeddings(len(tokenizer))
model.to("cuda")
model.train()

router = APIRouter()

# ìš”ì²­ ë°ì´í„° ëª¨ë¸
class LyricsRequest(BaseModel):
    text: str  # ì „ì²´ ê°€ì‚¬ ì „ë‹¬

# ê°€ì‚¬ í‰ê°€ API (BLEU, ROUGE, Perplexity)
@router.post("/evaluation/")
async def evaluate_lyrics(request: LyricsRequest):
    text = request.text

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    sentences = [sentence.strip() for sentence in text.split("\n") if sentence.strip()]

    # í‰ê°€ ì§€í‘œ ë¶ˆëŸ¬ì˜¤ê¸°
    bleu = load("bleu")
    rouge = load("rouge")

    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì¼ë¶€ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
    num_samples = 10
    test_samples = val_texts.sample(num_samples).tolist()

    # BLEU ë° ROUGE í‰ê°€
    references = []
    predictions = []

    for sample in test_samples:
        input_ids = tokenizer(sample, return_tensors="pt", truncation=True, max_length=512).input_ids.to("cuda")
        
        with torch.no_grad():
            output = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True)
        
        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
        
        references.append([sample])  # BLEU í‰ê°€ëŠ” ë‹¤ì¤‘ ì°¸ì¡° ê°€ëŠ¥í•˜ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ê¸°
        predictions.append(generated_text)

    # 4. BLEU ì ìˆ˜ ê³„ì‚°
    bleu_score = bleu.compute(predictions=predictions, references=references)
    print(f"BLEU Score: {bleu_score['bleu']:.4f}")

    # 5. ROUGE ì ìˆ˜ ê³„ì‚°
    rouge_score = rouge.compute(predictions=predictions, references=references)
    print(f"ROUGE Score: {rouge_score}")

    # 6. Perplexity(í˜¼ë€ë„) ê³„ì‚°
    def compute_perplexity(model, text):
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {key: value.to("cuda") for key, value in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs, labels=inputs["input_ids"])  # ğŸ”¥ labels ì¶”ê°€
        
        if outputs.loss is None:
            print("Warning: Model did not return a loss value.")
            return np.nan  # ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ NaN ë°˜í™˜
        
        loss = outputs.loss.item()
        return np.exp(loss)

    perplexities = [compute_perplexity(model, text) for text in test_samples]
    avg_perplexity = np.nanmean(perplexities)  # NaN ê°’ ì œì™¸í•˜ê³  í‰ê·  ê³„ì‚°
    print(f"Perplexity: {avg_perplexity:.4f}")


    # BLEU ì ìˆ˜ ë°˜ì˜¬ë¦¼ ì²˜ë¦¬
    rounded_bleu = {k: round(v, 4) if isinstance(v, (int, float)) else v for k, v in bleu_score.items()}

    # ROUGE ì ìˆ˜ ë°˜ì˜¬ë¦¼ ì²˜ë¦¬
    rounded_rouge = {k: round(v, 4) if isinstance(v, (int, float)) else v for k, v in rouge_score.items()}

    # ê²°ê³¼ ë°˜í™˜
    return {
        "bleu": rounded_bleu,
        "rouge": rounded_rouge,
        "perplexity": round(avg_perplexity, 4)
    }


    